{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ASL Complete Alphabet Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Faisal Mushayt\n",
    "# Myles Cork\n",
    "# Deemah Al Dulaijan\n",
    "# ---------------------------------------------------------------\n",
    "%matplotlib inline\n",
    "import keras\n",
    "import cv2\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.models import load_model\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myFrameDifferencing(prev, curr):\n",
    "    '''\n",
    "       Difference between last two frames \n",
    "    '''\n",
    "    dst = cv2.absdiff(prev, curr)\n",
    "    dst = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "    _, dst = cv2.threshold(dst, 50, 255, cv2.THRESH_BINARY)\n",
    "    return dst\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def myMotionEnergy(mh):\n",
    "    '''\n",
    "        Captures motion Energy by setting any pixel observed in the \n",
    "        last 3 frames to white \n",
    "    '''\n",
    "    mh0 = mh[0]\n",
    "    mh1 = mh[1]\n",
    "    mh2 = mh[2]\n",
    "    dst = np.zeros((mh0.shape[0], mh0.shape[1], 1), dtype = \"uint8\")\n",
    "    for i in range(mh0.shape[0]):\n",
    "        for j in range(mh0.shape[1]):\n",
    "            if mh0[i,j] == 255 or mh1[i,j] == 255 or mh2[i,j] == 255:\n",
    "                dst[i,j] = 255\n",
    "    return dst\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def getFrame():\n",
    "    '''\n",
    "        Records video and takes either static or dynamic sign.\n",
    "        For static, returns image of sign\n",
    "        For dynamic, returns motion energy image  \n",
    "    '''\n",
    "    \n",
    "    # Initiate video capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    \n",
    "    # While no command has been issued, take video\n",
    "    while True:\n",
    "        \n",
    "        # Get current frame\n",
    "        _, curr_frame = cap.read()\n",
    "        \n",
    "        # Check for command key\n",
    "        k = cv2.waitKey(1)\n",
    "        \n",
    "        # If esc hit, terminate prediction\n",
    "        if k%256 == 27:\n",
    "            print(\"Escape hit, closing...\")\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            sys.exit()\n",
    "            \n",
    "        # If 's' hit, take static sign\n",
    "        elif k%256 == ord('s'):\n",
    "            motion = False\n",
    "            img = cv2.resize(curr_frame, (224,224))\n",
    "            break\n",
    "        \n",
    "        # If 'd' hit, take dynamic sign (j or z)\n",
    "        elif k%256 == ord('d'):\n",
    "            motion = True\n",
    "            \n",
    "            # Record current frame\n",
    "            _, prev_frame = cap.read()\n",
    "                    \n",
    "            # Motion history set up\n",
    "            fMH1 = np.zeros((prev_frame.shape[0], prev_frame.shape[1], 1), dtype = \"uint8\")\n",
    "            fMH2 = fMH1.copy()\n",
    "            fMH3 = fMH1.copy()\n",
    "            myMotionHistory = deque([fMH1, fMH2, fMH3])             \n",
    "            while True:\n",
    "                # Get current frame\n",
    "                _, curr_frame = cap.read()\n",
    "                \n",
    "                # Keep track of frame differences\n",
    "                frameDest = myFrameDifferencing(prev_frame, curr_frame)                \n",
    "                myMotionHistory.popleft()\n",
    "                myMotionHistory.append(frameDest)\n",
    "\n",
    "                k = cv2.waitKey(1)\n",
    "                # If 'd' hit again, dynamic sign is done\n",
    "                if k%256 == ord('d'):\n",
    "                    break\n",
    "                \n",
    "                # Display frame\n",
    "                cv2.imshow(\"Detect Sign\", curr_frame)\n",
    "            \n",
    "            # Create motion energy image\n",
    "            img = myMotionEnergy(myMotionHistory)\n",
    "            break\n",
    "        \n",
    "        # If no command issued, do nothing\n",
    "        else: pass\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow(\"Detect Sign\", curr_frame)\n",
    "    \n",
    "    # Release cap and destroy all windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    # Return whether motion was indicated and the appropriate image\n",
    "    return motion, img\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def predictJZ(myMH):\n",
    "    '''\n",
    "        Differentiates between the motion energy of J and Z\n",
    "        by pattern matching the motion history to premade \n",
    "        templates\n",
    "    '''\n",
    "    # read in \"J\" templates and intialize variables\n",
    "    j_temp1 = cv2.imread(\"templates/j_1.jpg\", 0)\n",
    "    j_temp2 = cv2.imread(\"templates/j_2.jpg\", 0)\n",
    "    j_temp3 = cv2.imread(\"templates/j_3.jpg\", 0)\n",
    "    j_temp4 = cv2.imread(\"templates/j_4.jpg\", 0)\n",
    "    j_temp5 = cv2.imread(\"templates/j_5.jpg\", 0)\n",
    "    j_thresh = 0.35\n",
    "\n",
    "    # read in \"Z\" templates and initialize variables\n",
    "    z_temp1 = cv2.imread(\"templates/z_1.jpg\", 0)\n",
    "    z_temp2 = cv2.imread(\"templates/z_2.jpg\", 0)\n",
    "    z_temp3 = cv2.imread(\"templates/z_3.jpg\", 0)\n",
    "    z_temp4 = cv2.imread(\"templates/z_4.jpg\", 0)\n",
    "    z_temp5 = cv2.imread(\"templates/z_5.jpg\", 0)\n",
    "    z_thresh = 0.95\n",
    "\n",
    "    action = \"Nothing\"\n",
    "\n",
    "    # detect j\n",
    "    # Comparison results of 5 different j templates\n",
    "    jres1 = cv2.matchTemplate(myMH, j_temp1, cv2.TM_CCOEFF_NORMED)\n",
    "    jres2 = cv2.matchTemplate(myMH, j_temp2, cv2.TM_CCOEFF_NORMED)\n",
    "    jres3 = cv2.matchTemplate(myMH, j_temp3, cv2.TM_CCOEFF_NORMED)\n",
    "    jres4 = cv2.matchTemplate(myMH, j_temp4, cv2.TM_CCOEFF_NORMED)\n",
    "    jres5 = cv2.matchTemplate(myMH, j_temp5, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # find locations of points that are similar to the templates using a threshold\n",
    "    jloc1 = np.where( jres1 >= j_thresh)\n",
    "    jloc2 = np.where( jres2 >= j_thresh)\n",
    "    jloc3 = np.where( jres3 >= j_thresh)\n",
    "    jloc4 = np.where( jres4 >= j_thresh)\n",
    "    jloc5 = np.where( jres5 >= j_thresh)\n",
    "\n",
    "    # detect z\n",
    "    # Comparison results of 5 different j templates\n",
    "    zres1 = cv2.matchTemplate(myMH, z_temp1, cv2.TM_CCOEFF_NORMED)\n",
    "    zres2 = cv2.matchTemplate(myMH, z_temp2, cv2.TM_CCOEFF_NORMED)\n",
    "    zres3 = cv2.matchTemplate(myMH, z_temp3, cv2.TM_CCOEFF_NORMED)\n",
    "    zres4 = cv2.matchTemplate(myMH, z_temp4, cv2.TM_CCOEFF_NORMED)\n",
    "    zres5 = cv2.matchTemplate(myMH, z_temp5, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "    # find locations of points that are similar to the templates using a threshold\n",
    "    zloc1 = np.where( zres1 >= z_thresh)\n",
    "    zloc2 = np.where( zres2 >= z_thresh)\n",
    "    zloc3 = np.where( zres3 >= z_thresh)\n",
    "    zloc4 = np.where( zres4 >= z_thresh)\n",
    "    zloc5 = np.where( zres5 >= z_thresh)\n",
    "\n",
    "    # if there are similar points, classify the sign as J\n",
    "    flagj = []\n",
    "    for pt in zip(*jloc1[::-1]):\n",
    "        if pt is not None:\n",
    "            flagj.append(True)\n",
    "    for pt in zip(*jloc2[::-1]):\n",
    "        if pt is not None:\n",
    "            flagj.append(True)\n",
    "    for pt in zip(*jloc3[::-1]):\n",
    "        if pt is not None:\n",
    "            flagj.append(True)\n",
    "    for pt in zip(*jloc4[::-1]):\n",
    "        if pt is not None:\n",
    "            flagj.append(True)\n",
    "    for pt in zip(*jloc5[::-1]):\n",
    "        if pt is not None:\n",
    "            flagj.append(True)\n",
    "    \n",
    "    # if there are similar points, classify the sign as Z\n",
    "    flagz = []\n",
    "    for pt in zip(*zloc1[::-1]):\n",
    "        if pt is not None:\n",
    "            flagz.append(True)\n",
    "    for pt in zip(*zloc2[::-1]):\n",
    "        if pt is not None:\n",
    "            flagz.append(True)\n",
    "    for pt in zip(*zloc3[::-1]):\n",
    "        if pt is not None:\n",
    "            flagz.append(True)\n",
    "    for pt in zip(*zloc4[::-1]):\n",
    "        if pt is not None:\n",
    "            flagz.append(True)\n",
    "    for pt in zip(*zloc5[::-1]):\n",
    "        if pt is not None:\n",
    "            flagz.append(True)\n",
    "    if len(flagj) > len(flagz):\n",
    "        action = \"J\"\n",
    "    else:\n",
    "        action = \"Z\"\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ASL Recognizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLRecognition():\n",
    "    \n",
    "    def __init__(self, load = True):\n",
    "        ''' \n",
    "            Initializes ResNet model either randomly or load existing\n",
    "            \n",
    "            Params:\n",
    "                Loss = categorical crossentropy (log loss)\n",
    "                Optimizer = adam\n",
    "        '''\n",
    "        \n",
    "        if load == True:\n",
    "            \n",
    "            # Load model and history data\n",
    "            self.model = load_model('asl_model.h5')\n",
    "            with open('asl_history.json', 'r') as fp:\n",
    "                self.history = json.load(fp)\n",
    "            print(\"Model loaded\")\n",
    "            \n",
    "        else:\n",
    "            # Initialize ResNet50 model\n",
    "            self.history = dict()\n",
    "            self.model = ResNet50(weights = None, classes = 24)\n",
    "            \n",
    "            '''\n",
    "            The best result we found was without applying regularization\n",
    "            manually. We go into more detail on this in our report.\n",
    "            \n",
    "            \n",
    "            # Apply weight decay terms to l2 regularize\n",
    "            # Taken from https://jricheimer.github.io/keras/2019/02/06/keras-hack-1/\n",
    "            decay = 0.001\n",
    "            for layer in self.model.layers:\n",
    "                if isinstance(layer, keras.layers.Conv2D) or isinstance(layer, keras.layers.Dense):\n",
    "                    layer.add_loss(keras.regularizers.l2(decay)(layer.kernel))\n",
    "                if hasattr(layer, 'bias_regularizer') and layer.use_bias:\n",
    "                    layer.add_loss(keras.regularizers.l2(decay)(layer.bias))\n",
    "            '''\n",
    "            \n",
    "            # Configure the network with the Adam optimizer and logloss \n",
    "            self.model.compile(optimizer = \"Adam\", loss = \"categorical_crossentropy\", metrics=['accuracy'])\n",
    "            print(\"Model initialized\")\n",
    "            \n",
    "            \n",
    "        return\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def fitCNN(self, directory):\n",
    "        ''' input: filepath (directory) name\n",
    "            output: None\n",
    "            \n",
    "            Fits ResNet50 model onto provided data and evaluate metrics\n",
    "            \n",
    "            Parameters:\n",
    "                Validation percentage = 20%\n",
    "                Epochs = 50       \n",
    "        '''\n",
    "        \n",
    "        # Randomly split images into training and validation subsets using a 80:20 ratio\n",
    "        vsplit = 0.2\n",
    "        img_gen = image.ImageDataGenerator(validation_split = vsplit)\n",
    "        \n",
    "        # Generate training and validation data after preprocessing images\n",
    "        train_batch = img_gen.flow_from_directory(directory, target_size = (224, 224), subset = \"training\")\n",
    "        valid_batch = img_gen.flow_from_directory(directory, target_size = (224, 224), subset = \"validation\")\n",
    "        \n",
    "        # train the model on the data and record history\n",
    "        self.history = self.model.fit_generator(train_batch, epochs=50, steps_per_epoch = 1788, validation_data= valid_batch, validation_steps = 447)\n",
    "        self.history = self.history.history\n",
    "        return\n",
    "     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, img_path=None):\n",
    "        ''' input: Image path\n",
    "            output: None\n",
    "            \n",
    "             Print the predicted labels with the highest probability.\n",
    "             If no image path passed in, starts video frame that waits\n",
    "             for command:\n",
    "                 Esc: cancel prediction\n",
    "                 s: take static image for (a-y) prediction not including J\n",
    "                 d: take recording until d hit again, captures dynamic sign\n",
    "                    and predicts either J or Z\n",
    "        '''\n",
    "        \n",
    "        # Decide whether to predict from camera or existing image\n",
    "        if not img_path:\n",
    "            # Capture frame and decide how to proceed\n",
    "            motion, img = getFrame()\n",
    "        else:\n",
    "            # Process existing image\n",
    "            motion = False\n",
    "            img = image.load_img(img_path, target_size=(224, 224))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # If we observe motion, classify as j or z\n",
    "        if motion:\n",
    "            sign = predictJZ(img)\n",
    "            print('Predicted: ' + sign)\n",
    "            return\n",
    "        \n",
    "        # Otherwise, use ResNet50 to classify a-y (!= j)\n",
    "        else:\n",
    "            img = image.img_to_array(img)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            img = preprocess_input(img)\n",
    "            preds = self.model.predict(img)\n",
    "            print('Predicted:', self.decode_predictions(preds))        \n",
    "            \n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def decode_predictions(self, preds, top=3):\n",
    "        ''' input:\n",
    "                  preds -> array of prediction results from keras predict\n",
    "            output:\n",
    "                  decoded -> dictionary of 'top' labels and corresponding probailities\n",
    "        '''\n",
    "        \n",
    "        # Set labels\n",
    "        labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y']\n",
    "       \n",
    "        # Add the labels and their probabilities to a dictionary\n",
    "        decoded = dict()\n",
    "        for i in range(24): decoded[labels[i]] = preds[0][i]\n",
    "\n",
    "        # Obtain the \"top\" predictions and return them\n",
    "        preds[0].sort()\n",
    "        TOP = preds[0][-top:]\n",
    "        decoded = dict((k,v) for k,v in decoded.items() if v in TOP)\n",
    "        return decoded\n",
    "       \n",
    "    \n",
    "    def save(self):\n",
    "        ''' \n",
    "            Saves new model in H5 file and history in json file\n",
    "        '''\n",
    "        \n",
    "        self.model.save('asl_model_new.h5')\n",
    "        with open('asl_history_new.json', 'w') as fp:\n",
    "            json.dump(self.history, fp)\n",
    "            \n",
    "        print(\"Model saved\")\n",
    "        return\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
